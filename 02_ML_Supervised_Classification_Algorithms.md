|   # | ML Type    | Algorithm Type   | Algorithm                                          | Category                     | Category Rationale                                       | Real-World Use Case                                      | Input → Output                 | Assumptions                                                       | Strengths                                              | Weaknesses                                              | Equation                                      | Dependent variable(s)   | Independent variable(s)   | Model Parameters           | Hyperparameters             | Evaluation Metrics                                                                              | Overfitting Risk   | Underfitting Risk   | Interpretability   | Scalability   | Training Time Complexity   | Python Import                                                           | Historical Origin          |
|----:|:-----------|:-----------------|:---------------------------------------------------|:-----------------------------|:---------------------------------------------------------|:---------------------------------------------------------|:-------------------------------|:------------------------------------------------------------------|:-------------------------------------------------------|:--------------------------------------------------------|:----------------------------------------------|:------------------------|:--------------------------|:---------------------------|:----------------------------|:------------------------------------------------------------------------------------------------|:-------------------|:--------------------|:-------------------|:--------------|:---------------------------|:------------------------------------------------------------------------|:---------------------------|
|  15 | Supervised | Classification   | Logistic Regression                                | Pure Statistics              | Maximum Likelihood Estimation (MLE) of log-odds via IRLS | Predict customer churn from usage patterns               | Email features → Spam?         | Linearity in log-odds, independence, no perfect multicollinearity | Interpretable odds ratios, fast, probabilistic output  | Assumes linearity in logit, fails on complex boundaries | P(y=1) = 1/(1 + e^-(β₀ + βX))                 | y ∈ {0,1}               | X                         | β₀, β₁…βₚ                  | C, penalty                  | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix                                | Low                | High                | ★★★★★              | ★★★★★         | O(n p)                     | from sklearn.linear_model import LogisticRegression                     | Cox, 1958                  |
|  16 | Supervised | Classification   | Perceptron                                         | True ML: Linear              | Subgradient descent on misclassification                 | Learn logical AND/OR gate                                | Binary features → Class        | Linearly separable classes                                        | Simple, online learning                                | No margin, no convergence if not separable              | ŷ = sign(w·X + b)                             | y ∈ {−1,+1}             | X                         | w, b                       | eta0, max_iter              | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix                                | High               | High                | ★★★★★              | ★★★★★         | O(n p)                     | from sklearn.linear_model import Perceptron                             | Rosenblatt, 1958           |
|  17 | Supervised | Classification   | MLP (Multi-Layer Perceptron)                       | True ML: Neural              | Backpropagation on cross-entropy                         | Handwritten digit recognition (MNIST)                    | Image → Digit                  | None (universal approximator)                                     | Can model complex patterns                             | Black box, data-hungry, overfits                        | ŷ = softmax(W₂ σ(W₁ X + b₁) + b₂)             | y (one-hot)             | X                         | W₁, W₂, b₁, b₂             | hidden_layer_sizes, alpha   | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix                                | High               | Low                 | ★☆☆☆☆              | ★★★☆☆         | O(n d p)                   | from sklearn.neural_network import MLPClassifier                        | Rumelhart, 1986            |
|  18 | Supervised | Classification   | LDA (Linear Discriminant Analysis)                 | Pure Statistics              | Bayes classifier with equal covariance                   | Classify Iris flowers by measurements                    | Features → Species             | Multivariate normality, equal covariance across classes           | Optimal if assumptions hold, reduces dim               | Fails if covariance differs                             | δₖ(x) = xᵀΣ⁻¹μₖ − ½μₖᵀΣ⁻¹μₖ + log πₖ          | y ∈ {1…K}               | X                         | μₖ, Σ, πₖ                  | solver                      | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix                                | Low                | High                | ★★★★☆              | ★★★★☆         | O(p³)                      | from sklearn.discriminant_analysis import LinearDiscriminantAnalysis    | Fisher, 1936               |
|  19 | Supervised | Classification   | QDA (Quadratic Discriminant Analysis)              | Pure Statistics              | Bayes with class-specific covariance                     | Classify wine cultivar from chemical analysis            | Features → Wine type           | Multivariate normality, different covariance per class            | More flexible than LDA                                 | Needs more data per class                               | δₖ = −½ log|Σₖ| − ½(x−μₖ)ᵀΣₖ⁻¹(x−μₖ) + log πₖ | y ∈ {1…K}               | X                         | μₖ, Σₖ, πₖ                 | reg_param                   | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix                                | Medium             | High                | ★★★★☆              | ★★★☆☆         | O(k p³)                    | from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis | Friedman, 1989             |
|  20 | Supervised | Classification   | Decision Tree Classifier                           | True ML: Greedy Tree         | Greedy splits on Gini/entropy                            | Diagnose diabetes from blood tests                       | Symptoms → Disease             | No functional form                                                | Easy to visualize, handles mixed data                  | Overfits, unstable to small changes                     | ŷ = majority vote in leaf                     | y                       | X                         | Split rules, leaf classes  | max_depth, criterion        | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix, Feature Importance            | High               | Low                 | ★★★★☆              | ★★★★☆         | O(n log n)                 | from sklearn.tree import DecisionTreeClassifier                         | Quinlan, 1986              |
|  21 | Supervised | Classification   | Random Forest Classifier                           | True ML: Bagged Ensemble     | Bootstrap + majority vote                                | Credit card fraud detection                              | Transaction → Fraud?           | No assumptions                                                    | High accuracy, robust to noise                         | Less interpretable, slower                              | ŷ = mode{ treeₜ(X) }                          | y                       | X                         | All trees (splits + votes) | n_estimators, max_features  | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix, OOB Error, Feature Importance | Low                | Low                 | ★★★☆☆              | ★★★★☆         | O(T n log n)               | from sklearn.ensemble import RandomForestClassifier                     | Breiman, 2001              |
|  22 | Supervised | Classification   | AdaBoost Classifier                                | True ML: Boosting            | Sequential reweighting of weak learners                  | Email spam filtering                                     | Features → Spam?               | No assumptions                                                    | Boosts weak models to strong                           | Sensitive to outliers                                   | ŷ = sign(Σ αₜ hₜ(X))                          | y ∈ {−1,+1}             | X                         | αₜ, weak learners          | n_estimators, learning_rate | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix, Training Error, Test Error    | High               | Medium              | ★★★☆☆              | ★★★☆☆         | O(T n)                     | from sklearn.ensemble import AdaBoostClassifier                         | Freund & Schapire, 1996    |
|  23 | Supervised | Classification   | XGBoost Classifier                                 | True ML: Boosting            | 2nd-order Taylor + regularization                        | Predict ad click-through rate (CTR) in real-time bidding | Features → Click?              | No assumptions                                                    | State-of-the-art, handles missing values, parallelized | Complex tuning, memory-intensive                        | obj = Σ loss(y,ŷ) + Σ Ω(tree)                 | y                       | X                         | Trees, weights             | max_depth, eta, subsample   | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix, Feature Importance, Log-Loss  | High               | Low                 | ★★☆☆☆              | ★★★★★         | O(T n log n)               | import xgboost as xgb                                                   | Chen & Guestrin, 2016      |
|  24 | Supervised | Classification   | LightGBM Classifier                                | True ML: Boosting            | Histogram + leaf-wise growth                             | Predict disease risk from EHR data                       | 10M+ rows → Disease?           | No assumptions                                                    | Extremely fast, low memory, GOSS/EFB                   | Less control over tree structure                        | ŷ = Σ treeₜ(X)                                | y                       | X                         | Trees, histograms          | num_leaves, learning_rate   | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix, Feature Importance, Log-Loss  | High               | Low                 | ★★☆☆☆              | ★★★★★         | O(T n)                     | import lightgbm as lgb                                                  | Ke et al., 2017            |
|  25 | Supervised | Classification   | CatBoost Classifier                                | True ML: Boosting            | Ordered target encoding for categoricals                 | Predict telecom customer churn                           | Categorical + numeric → Churn? | No assumptions                                                    | Native categorical handling, no one-hot                | Slower startup (encoding)                               | ŷ = Σ treeₜ(X)                                | y                       | X                         | Trees, cat embeddings      | depth, cat_features         | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix, Feature Importance, Log-Loss  | High               | Low                 | ★★☆☆☆              | ★★★★☆         | O(T n log n)               | from catboost import CatBoostClassifier                                 | Prokhorenkova, 2018        |
|  26 | Supervised | Classification   | KNN (K-Nearest Neighbors)                          | True ML: Instance-Based      | Majority vote of k nearest                               | Recommend movie by similarity to others                  | New user → Segment             | Metric space, smooth boundaries                                   | No training, intuitive                                 | Slow prediction, curse of dim                           | ŷ = mode{ yᵢ : i ∈ Nₖ(x) }                    | y                       | X                         | None (stores data)         | k, metric                   | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix                                | Low                | Low                 | ★★★☆☆              | ★☆☆☆☆         | O(1) train, O(n) predict   | from sklearn.neighbors import KNeighborsClassifier                      | Fix & Hodges, 1951         |
|  27 | Supervised | Classification   | SVM Classifier (Support Vector Machine Classifier) | True ML: Kernel Optimization | Max margin + hinge loss                                  | Classify movie reviews as positive/negative              | Text → Sentiment               | Kernel maps to linear                                             | Max margin → robust, non-linear                        | Opaque, slow, sensitive to scale                        | ŷ = sign(w·ϕ(X) + b)                          | y ∈ {−1,+1}             | X                         | w, b, support vectors      | C, kernel                   | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix, Support Vectors               | Medium             | High (linear)       | ★★☆☆☆              | ★★☆☆☆         | O(n²–n³)                   | from sklearn.svm import SVC                                             | Cortes & Vapnik, 1995      |
|  28 | Supervised | Classification   | Naive Bayes                                        | Pure Statistics              | Bayes theorem + independence                             | Email spam filtering using word counts                   | Words → Spam?                  | Conditional independence of features                              | Fast, works with little data, stable                   | Fails if features are correlated                        | P(y|X) ∝ P(y) Π P(Xᵢ|y)                       | y                       | X₁, …, Xₙ                 | P(y), P(Xᵢ|y)              | var_smoothing               | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix                                | Low                | High                | ★★★★☆              | ★★★★★         | O(n p)                     | from sklearn.naive_bayes import GaussianNB                              | Bayes, 1763                |
|  29 | Supervised | Classification   | Gaussian Processes                                 | True ML: Bayesian            | Kernel-based posterior                                   | Predict machine failure from vibration                   | Sensor → Fault?                | Smoothness, stationarity                                          | Uncertainty quantification, flexible                   | O(n³), not scalable                                     | f ~ GP(0, k(X,X'))                            | y                       | X                         | Kernel hyperparameters     | kernel                      | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Confusion Matrix                                | Low                | High                | ★★☆☆☆              | ★☆☆☆☆         | O(n³)                      | from sklearn.gaussian_process import GaussianProcessClassifier          | Rasmussen & Williams, 2006 |