|   # | ML Type      | Algorithm Type           | Algorithm                                                            | Category                        | Category Rationale                                                | Real-World Use Case                                        | Input → Output          | Assumptions                                  | Strengths                                  | Weaknesses                               | Equation                        | Dependent variable(s)   | Independent variable(s)   | Model Parameters               | Hyperparameters               | Evaluation Metrics                                                                                                                                 | Overfitting Risk   | Underfitting Risk      | Interpretability   | Scalability   | Training Time Complexity   | Python Import                                       | Historical Origin                 |
|----:|:-------------|:-------------------------|:---------------------------------------------------------------------|:--------------------------------|:------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------|:---------------------------------------------|:-------------------------------------------|:-----------------------------------------|:--------------------------------|:------------------------|:--------------------------|:-------------------------------|:------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------|:-----------------------|:-------------------|:--------------|:---------------------------|:----------------------------------------------------|:----------------------------------|
|  30 | Unsupervised | Dimensionality Reduction | PCA (Principal Component Analysis)                                   | Pure Statistics                 | Eigen-decomposition of covariance matrix to maximize variance     | Reduce gene expression data to top 2 PCs for visualization | Data → Components       | Linearity, orthogonality, uncorrelated noise | Max variance, optimal linear, fast         | Only linear, sensitive to scale          | X = T Pᵀ + E                    | None                    | X ∈ ℝⁿˣᵖ                  | P (principal axes), T (scores) | n_components                  | Explained Variance Ratio, Reconstruction Error, Cumulative Variance Explained                                                                      | Low                | High (if non-linear)   | ★★★★★              | ★★★★★         | O(min(n,p)³)               | from sklearn.decomposition import PCA               | Pearson, 1901                     |
|  31 | Unsupervised | Dimensionality Reduction | t-SNE (t-Distributed Stochastic Neighbor Embedding)                  | True ML: Manifold               | Minimizes KL divergence between high-dim and low-dim similarities | Visualize 1000-dimensional word embeddings                 | Points → 2D map         | Local structure preserved, manifold          | Stunning 2D/3D plots, reveals clusters     | Slow, stochastic, not for new data       | min KL(P ‖ Q)                   | None                    | X                         | Embedding Y ∈ ℝⁿˣ²             | perplexity, learning_rate     | Perplexity, KL-Divergence, Visual Inspection (Cluster Separation, Local vs Global Structure)                                                       | N/A                | N/A                    | ★★☆☆☆              | ★☆☆☆☆         | O(n²)                      | from sklearn.manifold import TSNE                   | van der Maaten & Hinton, 2008     |
|  32 | Unsupervised | Dimensionality Reduction | UMAP (Uniform Manifold Approximation and Projection)                 | True ML: Manifold               | Preserves fuzzy topological structure via cross-entropy           | Visualize single-cell RNA-seq clusters                     | Points → Embedding      | Topological structure                        | Fast, scalable, preserves global structure | Less theoretical grounding               | min CE(p ‖ q)                   | None                    | X                         | Embedding Y                    | n_neighbors, min_dist         | Perplexity, Visual Inspection (Cluster Separation, Local vs Global Structure), Trustworthiness, Continuity                                         | N/A                | N/A                    | ★★★☆☆              | ★★★★☆         | O(n log n)                 | import umap                                         | McInnes, Healy & Melville, 2018   |
|  33 | Unsupervised | Clustering               | K-Means                                                              | True ML: Iterative Optimization | Lloyd’s algorithm: minimize within-cluster variance               | Segment retail customers by purchase behavior              | Customers → Segment     | Spherical, equal variance clusters           | Fast, scalable, simple                     | Sensitive to init, fails on non-convex   | min Σ‖xᵢ − μₖ‖²                 | None                    | X                         | μ₁, …, μₖ (centroids)          | k, init                       | Silhouette Score, Davies-Bouldin Index, Inertia (Within-cluster Sum of Squares), Visual Inspection (Cluster Shape)                                 | Low                | High (wrong k)         | ★★★☆☆              | ★★★★★         | O(n k i)                   | from sklearn.cluster import KMeans                  | Lloyd, 1957                       |
|  34 | Unsupervised | Clustering               | DBSCAN (Density-Based Spatial Clustering of Applications with Noise) | True ML: Density-Based          | Core points + ε-neighborhood + MinPts                             | Detect traffic jams from GPS traces                        | GPS points → Clusters   | Density-connected regions                    | No K, handles noise, arbitrary shape       | Sensitive to ε, fails in varying density | Core(i): |N_ε(xᵢ)| ≥ MinPts     | None                    | X                         | Labels (core, border, noise)   | eps, min_samples              | Silhouette Score, Davies-Bouldin Index, Number of Noise Points, Visual Inspection (Cluster Density and Shape)                                      | Low                | Low                    | ★★★★☆              | ★★☆☆☆         | O(n log n) with index      | from sklearn.cluster import DBSCAN                  | Ester, Kriegel, Sander & Xu, 1996 |
|  35 | Unsupervised | Clustering               | Mean Shift                                                           | True ML: Density-Based          | Iterative shift to mode of kernel density                         | Image segmentation (color clustering)                      | Pixels → Segments       | Smooth density, bandwidth                    | No K, finds arbitrary modes                | Slow, bandwidth critical                 | m(x) = (Σ wᵢ xᵢ) / (Σ wᵢ)       | None                    | X                         | Modes (cluster centers)        | bandwidth                     | Silhouette Score, Cluster Count, Visual Inspection (Cluster Location and Spread)                                                                   | Low                | High (wrong bandwidth) | ★★★☆☆              | ★☆☆☆☆         | O(n²)                      | from sklearn.cluster import MeanShift               | Fukunaga & Hostetler, 1975        |
|  36 | Unsupervised | Clustering               | Hierarchical Clustering                                              | True ML: Greedy Agglomerative   | Bottom-up merging using linkage                                   | Build evolutionary tree from DNA                           | Species → Taxonomy      | None                                         | No K needed, visual dendrogram             | O(n³), not scalable                      | d(Cᵢ,Cⱼ) = linkage              | None                    | X                         | Dendrogram (merge order)       | linkage, distance             | Cophenetic Correlation Coefficient, Silhouette Score, Dendrogram Visual Inspection (Cluster Structure and Hierarchy)                               | Low                | Low                    | ★★★★☆              | ★★☆☆☆         | O(n³)                      | from sklearn.cluster import AgglomerativeClustering | Ward, 1963                        |
|  37 | Unsupervised | Clustering               | GMM (Gaussian Mixture Model)                                         | Pure Statistics                 | EM for mixture of Gaussians                                       | Speaker diarization in meetings                            | Audio → Speaker         | Data from Gaussian mixture                   | Soft clustering, probabilistic             | Local minima, needs init                 | p(x) = Σ πₖ N(x|μₖ,Σₖ)          | None                    | X                         | πₖ, μₖ, Σₖ                     | n_components, covariance_type | Log-Likelihood, AIC, BIC, Silhouette Score, Visual Inspection (Elliptical Cluster Fit)                                                             | Medium             | High (wrong k)         | ★★★★☆              | ★★★☆☆         | O(n k p² i)                | from sklearn.mixture import GaussianMixture         | Dempster, Laird & Rubin, 1977     |
|  38 | Unsupervised | Anomaly Detection        | Isolation Forest                                                     | True ML: Ensemble               | Random partitioning isolates anomalies faster                     | Detect credit card fraud                                   | Transactions → Fraud?   | Anomalies are few and different              | Fast, scalable, no distance                | Less sensitive to scale                  | path length h(x)                | None                    | X                         | Trees, average path            | n_estimators, contamination   | Anomaly Score (Path Length), ROC-AUC / Precision-Recall (if labels available), Visual Inspection (Outlier Distribution)                            | Low                | Low                    | ★★★☆☆              | ★★★★★         | O(T n log n)               | from sklearn.ensemble import IsolationForest        | Liu, Ting & Zhou, 2008            |
|  39 | Unsupervised | Association Rule Mining  | Apriori (Association Rule Learning)                                  | True ML: Heuristic Pattern      | Candidate generation + pruning by support                         | Recommend products in supermarket                          | Cart → "Bread → Butter" | Frequent co-occurrence                       | Efficient pruning, interpretable           | Memory-heavy for dense data              | support(A→B) = P(A∪B)           | None                    | Transactions              | Frequent itemsets, rules       | min_support, min_confidence   | Support, Confidence, Lift, Leverage, Conviction                                                                                                    | Low                | Low                    | ★★★★☆              | ★★★☆☆         | O(2^d) worst-case          | from mlxtend.frequent_patterns import apriori       | Agrawal & Srikant, 1994           |
|  40 | Unsupervised | Association Rule Mining  | FP-Growth (Frequent Pattern Growth)                                  | True ML: Heuristic Pattern      | FP-tree + recursive mining                                        | Mine large transaction databases                           | Cart → "Diaper → Beer"  | Frequent patterns                            | No candidate sets, faster than A-priori    | Complex tree structure                   | support via conditional FP-tree | None                    | Transactions              | FP-tree, conditional patterns  | min_support                   | Support, Confidence, Lift, Execution Time, Memory Efficiency                                                                                       | Low                | Low                    | ★★★☆☆              | ★★★★☆         | O(n)                       | from mlxtend.frequent_patterns import fpgrowth      | Han, Pei & Yin, 2000              |
|  41 | Unsupervised | Neural                   | Autoencoder                                                          | True ML: Neural                 | Minimize reconstruction error                                     | Denoise handwritten digits                                 | Image → Latent          | Manifold structure                           | Non-linear dim reduction, feature learning | Black box, needs GPU                     | x̂ = dec(enc(x))                 | None                    | X                         | Encoder/decoder weights        | latent_dim, layers            | Reconstruction Error (MSE, MAE), Latent Space Visualization (Cluster Separation), Anomaly Score (via Error Threshold), Trustworthiness, Continuity | High               | Low                    | ★★☆☆☆              | ★★★☆☆         | O(n d i)                   | from tensorflow.keras import layers                 | Hinton & Salakhutdinov, 2006      |