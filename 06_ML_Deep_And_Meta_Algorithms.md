|   # | ML Type   | Algorithm Type   | Algorithm                           | Category          | Category Rationale                                                    | Real-World Use Case                         | Input → Output           | Assumptions                             | Strengths                                    | Weaknesses                        | Equation                               | Dependent variable(s)   | Independent variable(s)   | Model Parameters                | Hyperparameters               | Evaluation Metrics                                                                                                                                                                           | Overfitting Risk   | Underfitting Risk   | Interpretability   | Scalability   | Training Time Complexity   | Python Import                                   | Historical Origin                  |
|----:|:----------|:-----------------|:------------------------------------|:------------------|:----------------------------------------------------------------------|:--------------------------------------------|:-------------------------|:----------------------------------------|:---------------------------------------------|:----------------------------------|:---------------------------------------|:------------------------|:--------------------------|:--------------------------------|:------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------|:--------------------|:-------------------|:--------------|:---------------------------|:------------------------------------------------|:-----------------------------------|
|  49 | Deep      | Vision           | CNN (Convolutional Neural Network)  | True ML: Deep     | Local receptive fields + weight sharing extract hierarchical features | Classify cats vs dogs in photos             | Image → Label            | Stationary patterns, local connectivity | State-of-the-art vision, parameter efficient | Needs GPU, data-hungry, black box | y = f(conv(X; W) + b)                  | y (class or value)      | X (image)                 | Filters W, biases b             | filters, kernel_size, strides | Accuracy, Precision, Recall, F1-Score, Cross-Entropy Loss, ROC-AUC, Confusion Matrix, Top-K Accuracy, Visual Inspection (Feature Maps / Grad-CAM)                                            | High               | Low                 | ★☆☆☆☆              | ★★★★☆         | O(n · d · f²) per layer    | from tensorflow.keras import layers             | LeCun et al., 1989                 |
|  50 | Deep      | Sequence         | RNN (Recurrent Neural Network)      | True ML: Deep     | Recurrent hidden state captures temporal dependencies                 | Generate Shakespeare-style text             | Text → Next word         | Markovian dynamics                      | Models sequences                             | Vanishing/exploding gradients     | hₜ = σ(W_{xh} xₜ + W_{hh} h_{t−1} + b) | yₜ                      | x₁…xₜ                     | W_{xh}, W_{hh}, b               | units, return_sequences       | Accuracy (for classification), Perplexity (for language modeling), Cross-Entropy Loss, Sequence Prediction Error (MSE / MAE), Gradient Stability (Vanishing/Exploding Check)                 | High               | Low                 | ★★☆☆☆              | ★★★☆☆         | O(T · n · h²)              | from tensorflow.keras import layers             | Rumelhart, Hinton & Williams, 1986 |
|  51 | Deep      | Sequence         | LSTM (Long Short-Term Memory)       | True ML: Deep     | Gated memory cells prevent vanishing gradients                        | Analyze movie review sentiment              | Sentence → Sentiment     | Long-term dependencies                  | Long memory, robust to gaps                  | Complex, slower                   | fₜ, iₜ, oₜ, cₜ with σ and tanh         | yₜ                      | x₁…xₜ                     | Gate weights, cell state        | units, dropout                | Accuracy, Perplexity, MSE / MAE (for regression or forecasting), Cross-Entropy Loss, Sequence Length Performance, Training Stability (Gradient Norms)                                        | High               | Low                 | ★★☆☆☆              | ★★★☆☆         | O(T · n · h)               | from tensorflow.keras import layers             | Hochreiter & Schmidhuber, 1997     |
|  52 | Deep      | Attention        | Transformer                         | True ML: Deep     | Self-attention computes weighted context                              | Translate English to French (WMT)           | Text → Text              | Sequence structure                      | Parallel, captures long-range                | O(n²), data-hungry                | Attention(Q,K,V) = softmax(QKᵀ/√d)V    | y                       | x₁…xₙ                     | W_Q, W_K, W_V, FFN              | n_layers, d_model, n_heads    | Accuracy, Perplexity (for NLP), BLEU / ROUGE / METEOR (for text generation or translation), Cross-Entropy Loss, Attention Visualization (Interpretability), Training Efficiency (Throughput) | High               | Low                 | ★☆☆☆☆              | ★★★★★         | O(n² · d) per layer        | from transformers import AutoModel              | Vaswani et al., 2017               |
|  53 | Meta      | Ensemble         | Stacking (Stacked Generalization)   | True ML: Meta     | Train meta-learner on base model outputs                              | Win Kaggle competitions with diverse models | Predictions → Final      | Base models are diverse                 | Often best performance                       | Complex, risk of leakage          | ŷ = h(f₁(X), f₂(X), …, fₖ(X))          | y                       | X                         | Base models + meta h            | meta_learner, cv              | Accuracy, F1-Score, ROC-AUC, Cross-Validation Score (Base vs Meta-Learner), Out-of-Fold Error, Model Diversity (Correlation between base learners)                                           | Medium             | Low                 | ★★☆☆☆              | ★★★☆☆         | O(∑ training of bases)     | from sklearn.ensemble import StackingClassifier | Wolpert, 1992                      |
|  54 | Meta      | Ensemble         | Bagging (Bootstrap Aggregation)     | True ML: Ensemble | Bootstrap aggregating reduces variance                                | Reduce variance in unstable trees           | Data → Stable prediction | IID samples                             | Simple, parallel, stable                     | Less gain on stable models        | ŷ = (1/T) Σ f^*(X)                     | y                       | X                         | Ensemble of bootstrapped models | n_estimators, max_samples     | Accuracy, F1-Score, ROC-AUC, Variance Reduction (Compared to Base Model), Out-of-Bag (OOB) Error Estimate, Ensemble Stability                                                                | Low                | Low                 | ★★★★☆              | ★★★★☆         | O(T · n)                   | from sklearn.ensemble import BaggingClassifier  | Breiman, 1996                      |
|  55 | Meta      | Ensemble         | Voting (Ensemble Voting Classifier) | True ML: Ensemble | Majority (class) or average (reg) vote                                | Medical diagnosis via expert systems        | Models → Consensus       | Models are independent                  | Simple, robust                               | No synergy if correlated          | ŷ = mode{fᵢ(X)} or avg                 | y                       | X                         | Individual models               | voting='hard'/'soft', weights | Accuracy, Precision, Recall, F1-Score, ROC-AUC, Majority Agreement Rate, Diversity of Base Models (Prediction Correlation)                                                                   | Low                | Low                 | ★★★★☆              | ★★★★☆         | O(∑ inference time)        | from sklearn.ensemble import VotingClassifier   | Littlestone & Warmuth, 1989        |