|   # | ML Type         | Algorithm Type   | Algorithm                                                          | Category              | Category Rationale                                               | Real-World Use Case                                                | Input → Output               | Assumptions                                         | Strengths                              | Weaknesses                         | Equation                                                    | Dependent variable(s)   | Independent variable(s)   | Model Parameters                    | Hyperparameters           | Evaluation Metrics                                                                                                                                                               | Overfitting Risk         | Underfitting Risk    | Interpretability   | Scalability   | Training Time Complexity   | Python Import                                              | Historical Origin           |
|----:|:----------------|:-----------------|:-------------------------------------------------------------------|:----------------------|:-----------------------------------------------------------------|:-------------------------------------------------------------------|:-----------------------------|:----------------------------------------------------|:---------------------------------------|:-----------------------------------|:------------------------------------------------------------|:------------------------|:--------------------------|:------------------------------------|:--------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------|:---------------------|:-------------------|:--------------|:---------------------------|:-----------------------------------------------------------|:----------------------------|
|  56 | Semi-Supervised | Wrapper          | Self-Training                                                      | True ML: Iterative    | Train base on labeled, pseudo-label confident unlabeled, retrain | Classify web pages as relevant/irrelevant with few human labels    | Labeled + unlabeled → Labels | High-confidence predictions are correct, smoothness | Simple, leverages any supervised base  | Propagates errors if initial wrong | ŷ_u = argmax P(y|x_u; θ); θ ← argmin L(labeled + pseudo)    | y (partial)             | X_l ∪ X_u                 | Base model θ + pseudo-labels        | threshold, base_estimator | Accuracy (on labeled + pseudo-labeled data), F1-Score, Precision, Recall, Pseudo-Label Confidence Distribution, Label Noise Ratio, Learning Curve (Performance vs Iterations)    | High (error propagation) | Medium               | ★★★☆☆              | ★★★☆☆         | O(T · n_base)              | from sklearn.semi_supervised import SelfTrainingClassifier | Yarowsky, 1995              |
|  57 | Semi-Supervised | Multi-View       | Co-Training                                                        | True ML: Multi-View   | Two views; each trains on one, labels for the other              | Categorize web pages (text view + hyperlinks view) with few labels | Text + images → Category     | Views are independent, sufficient                   | Works with multi-modal data            | Needs redundant views              | θ₁ ← L(X₁_l, y); θ₂ ← L(X₂_l, y); pseudo from confident     | y (partial)             | X₁_l ∪ X₂_u (views)       | View models θ₁, θ₂                  | threshold, base_views     | Accuracy (Combined and Individual Classifiers), Agreement Rate between Classifiers, F1-Score, Label Consistency, Pseudo-Label Quality, Convergence Rate                          | Medium                   | High (view mismatch) | ★★★☆☆              | ★★☆☆☆         | O(T · n_view)              | Custom (sklearn base)                                      | Blum & Mitchell, 1998       |
|  58 | Semi-Supervised | Graph-Based      | Label Propagation                                                  | True ML: Graph        | Harmonic function on graph Laplacian                             | Propagate tags in social network with few seeds                    | Nodes → Labels               | Manifold (smooth labels on graph)                   | Non-parametric, handles irregular data | Sensitive to graph construction    | F = argmin Σ F_u - F_v² + Σ (F_l - y_l)²                    | y (partial)             | X_l ∪ X_u (graph)         | Graph labels F                      | gamma, kernel             | Accuracy (on validation or held-out labeled data), Label Smoothness, Consistency Score (Graph Structure Alignment), Number of Correctly Propagated Labels, Convergence Stability | Low                      | High (poor graph)    | ★★★★☆              | ★★☆☆☆         | O(n³)                      | from sklearn.semi_supervised import LabelPropagation       | Zhu & Ghahramani, 2002      |
|  59 | Semi-Supervised | Generative       | Generative Gaussian Mixture                                        | Pure Statistics       | EM on labeled + unlabeled for GMM                                | Classify handwritten digits with few labels + many unlabeled       | Images → Class               | Data from Gaussian mixtures                         | Probabilistic, incorporates priors     | Assumes Gaussian, local optima     | p(x,y) = Σ π_k N(x|μ_k,Σ_k) P(y|k)                          | y (partial)             | X_l ∪ X_u                 | π_k, μ_k, Σ_k                       | n_components              | Log-Likelihood, Accuracy (on labeled subset), AIC / BIC, Cluster Purity, Pseudo-Label Accuracy, Convergence Rate                                                                 | Medium                   | High (non-Gaussian)  | ★★★★☆              | ★★★☆☆         | O(n k p² i)                | Custom (sklearn GMM)                                       | Corduneanu & Jaakkola, 2002 |
|  60 | Semi-Supervised | Graph-Based      | Transductive SVM (TSVM)                                            | True ML: Kernel Opt.  | Max margin on labeled + unlabeled labels                         | Text classification with few labeled docs                          | Documents → Relevant?        | Low-density separation                              | Tight bound on unlabeled               | Non-convex, slow                   | min ½‖w‖² + C Σ ξ_i s.t. y_i (w·ϕ(x_i)+b) ≥ 1-ξ_i           | y (partial, inferred)   | X_l ∪ X_u                 | w, b, support vectors, inferred y_u | C, kernel                 | Accuracy (on labeled and unlabeled samples), Margin Maximization, Generalization Gap, Convergence Rate, Precision / Recall                                                       | High                     | Medium               | ★★☆☆☆              | ★★☆☆☆         | O(n³)                      | Custom (sklearn SVM)                                       | Joachims, 1999              |
|  61 | Semi-Supervised | Consistency      | Consistency Regularization                                         | True ML: Perturbation | Minimize loss on labeled + consistent on unlabeled perturbations | Semi-supervised image classification (CIFAR-10 with 4K labels)     | Images → Class               | Invariance to perturbations (smoothness)            | Improves with more unlabeled, simple   | Needs augmentation design          | L = L_labeled + λ E_{x_u~p(x)} [‖f(x_u) - f(x_u + noise)‖²] | y (partial)             | X_l ∪ X_u                 | Model f(θ)                          | λ, augmentations          | Accuracy, F1-Score, Consistency Loss (under data augmentation or perturbation), Pseudo-Label Confidence, Calibration Error, Learning Stability                                   | Medium                   | Low                  | ★★★☆☆              | ★★★☆☆         | O(n i)                     | Custom (e.g., torch)                                       | Chapelle et al., 2005       |
|  62 | Semi-Supervised | Generative       | VAE for SSL (Variational Autoencoder for Semi-Supervised Learning) | True ML: Generative   | Variational lower bound on labeled + marginal unlabeled          | Generate synthetic labeled data for rare diseases                  | Images → Augmented labels    | Latent Gaussian, disentangled                       | Handles missing labels, generative     | Complex, mode collapse             | ELBO = E[log p(x|z,y)] - KL(q(z|x,y) ‖ p(z))                | y (partial)             | X_l ∪ X_u                 | q(z|x,y), p(x|z,y)                  | latent_dim, β             | Classification Accuracy (on labeled subset), Reconstruction Loss, ELBO (Evidence Lower Bound), Latent Space Clustering Quality, Consistency Loss, F1-Score                       | High                     | Low                  | ★★☆☆☆              | ★★★☆☆         | O(n d i)                   | from torch import nn                                       | Kingma & Welling, 2013      |