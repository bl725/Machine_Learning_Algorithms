|   # | ML Type    | Algorithm Type   | Algorithm                       | Category                     | Category Rationale                                                          | Real-World Use Case                                     | Input → Output                | Assumptions                                                                        | Strengths                                                 | Weaknesses                                                   | Equation                           | Dependent variable(s)   | Independent variable(s)   | Model Parameters               | Hyperparameters              | Evaluation Metrics                                                                   | Overfitting Risk   | Underfitting Risk   | Interpretability   | Scalability   | Training Time Complexity   | Python Import                                                           | Historical Origin       |
|----:|:-----------|:-----------------|:--------------------------------|:-----------------------------|:----------------------------------------------------------------------------|:--------------------------------------------------------|:------------------------------|:-----------------------------------------------------------------------------------|:----------------------------------------------------------|:-------------------------------------------------------------|:-----------------------------------|:------------------------|:--------------------------|:-------------------------------|:-----------------------------|:-------------------------------------------------------------------------------------|:-------------------|:--------------------|:-------------------|:--------------|:---------------------------|:------------------------------------------------------------------------|:------------------------|
|   1 | Supervised | Regression       | Linear Regression               | Pure Statistics              | Closed-form Ordinary Least Squares (OLS) minimizes sum of squared residuals | Predict house price from square footage                 | House size → Price            | Linearity, independence, homoscedasticity, normality of errors                     | Simple, exact, global optimum, interpretable coefficients | Cannot model non-linear relationships, sensitive to outliers | y = β₀ + β₁X + ε                   | y (continuous)          | X (1+ features)           | β₀ (intercept), β₁…βₚ (slopes) | fit_intercept, normalize     | MAE, MSE, RMSE, R², Adj. R², F-statistic                                             | Low                | High                | ★★★★★              | ★★★★★         | O(n p)                     | from sklearn.linear_model import LinearRegression                       | Gauss, 1809             |
|   2 | Supervised | Regression       | Multilinear Regression          | Pure Statistics              | OLS with multiple predictors                                                | Predict employee salary from experience, education, age | Size, rooms, age → Price      | Linearity, no perfect multicollinearity, independence, homoscedasticity, normality | Interpretable feature importance, fast                    | Multicollinearity inflates variance, assumes linearity       | y = β₀ + β₁X₁ + … + βₚXₚ + ε       | y                       | X₁, …, Xₚ                 | β₀, β₁, …, βₚ                  | fit_intercept                | MAE, MSE, RMSE, R², Adj. R², F-statistic                                             | Low                | High                | ★★★★★              | ★★★★★         | O(n p²)                    | from sklearn.linear_model import LinearRegression                       | Legendre, 1805          |
|   3 | Supervised | Regression       | Polynomial Regression           | Pure Statistics              | OLS on transformed features (X, X², …)                                      | Model seasonal temperature trends                       | Time → Sales                  | Linearity in polynomial basis, no overfitting in degree                            | Captures non-linear trends                                | High-degree → numerical instability, overfitting             | y = β₀ + β₁X + β₂X² + … + βₖXᵏ + ε | y                       | X, X², …, Xᵏ              | β₀, β₁, …, βₖ                  | degree                       | MAE, MSE, RMSE, R², Adj. R², F-statistic                                             | High               | Low                 | ★★★★☆              | ★★★★☆         | O(n p²)                    | from sklearn.preprocessing import PolynomialFeatures + LinearRegression | Weierstrass, 1885       |
|   4 | Supervised | Regression       | Ridge Regression                | Pure Statistics              | OLS + L2 penalty shrinks coefficients                                       | Predict blood pressure from BMI, age, genetics          | Features → Value              | Linearity, multicollinearity expected                                              | Stable in high-dim, prevents overfitting                  | All features kept                                            | min ‖y−Xβ‖² + λ‖β‖²                | y                       | X                         | β₀, β₁…βₚ (shrunk)             | alpha                        | MAE, MSE, RMSE, R², Adj. R², F-statistic, Alpha                                      | Low                | High                | ★★★★★              | ★★★★★         | O(n p²)                    | from sklearn.linear_model import Ridge                                  | Hoerl & Kennard, 1970   |
|   5 | Supervised | Regression       | Lasso Regression                | Pure Statistics              | OLS + L1 penalty → sparsity                                                 | Gene selection in genomics                              | Features → Value              | Linearity, sparsity in truth                                                       | Automatic feature selection                               | Some relevant features may be dropped                        | min ‖y−Xβ‖² + λ‖β‖₁                | y                       | X                         | β₀, β₁…βₚ (many zero)          | alpha                        | MAE, MSE, RMSE, R², Adj. R², F-statistic, Alpha                                      | Low                | High                | ★★★★★              | ★★★★★         | O(n p²)                    | from sklearn.linear_model import Lasso                                  | Tibshirani, 1996        |
|   6 | Supervised | Regression       | Elastic Net                     | Pure Statistics              | Combines L1 + L2                                                            | Predict stock return with 1000+ factors                 | Features → Value              | Linearity, grouped features                                                        | Handles correlated groups                                 | Complex tuning                                               | min ‖y−Xβ‖² + λ₁‖β‖₁ + λ₂‖β‖²      | y                       | X                         | β₀, β₁…βₚ (sparse + shrunk)    | l1_ratio, alpha              | MAE, MSE, RMSE, R², Adj. R², F-statistic, Alpha                                      | Low                | High                | ★★★★☆              | ★★★★☆         | O(n p²)                    | from sklearn.linear_model import ElasticNet                             | Zou & Hastie, 2005      |
|   7 | Supervised | Regression       | Decision Tree Regression        | True ML: Greedy Tree         | Greedy recursive partitioning on MSE                                        | Predict length of hospital stay                         | Patient data → Recovery time  | No functional form                                                                 | Handles non-linearity, interactions                       | Unstable, overfits                                           | y = cᵢ in region Rᵢ                | y                       | X                         | Split points, leaf values      | max_depth, min_samples_split | MAE, MSE, RMSE, R², Adj. R², Feature importance                                      | High               | Low                 | ★★★★☆              | ★★★★☆         | O(n log n)                 | from sklearn.tree import DecisionTreeRegressor                          | Breiman, 1984           |
|   8 | Supervised | Regression       | SVR (Support Vector Regression) | True ML: Kernel Optimization | ε-insensitive loss + kernel                                                 | Predict daily stock movement                            | Stock features → Return       | Kernel maps to linear                                                              | Robust to outliers, non-linear                            | Hard to tune, slow                                           | y = w·ϕ(X) + b                     | y                       | X                         | w, b, support vectors          | C, epsilon, kernel           | MAE, MSE, RMSE, R², Adj. R², C (Regularization parameter)                            | Medium             | High (linear)       | ★★☆☆☆              | ★★☆☆☆         | O(n²–n³)                   | from sklearn.svm import SVR                                             | Vapnik, 1995            |
|   9 | Supervised | Regression       | Random Forest Reg.              | True ML: Bagged Ensemble     | Bootstrap + average trees                                                   | Predict energy consumption in buildings                 | Sensor data → Temperature     | No assumptions                                                                     | High accuracy, robust                                     | Slower, less interpretable                                   | ŷ = (1/T) Σ treeₜ(X)               | y                       | X                         | All trees (splits + leaves)    | n_estimators, max_depth      | MAE, MSE, RMSE, R², Adj. R², Feature importance, OOB Error (Out-of-Bag)              | Low                | Low                 | ★★★☆☆              | ★★★★☆         | O(T n log n)               | from sklearn.ensemble import RandomForestRegressor                      | Breiman, 2001           |
|  10 | Supervised | Regression       | AdaBoost Reg.                   | True ML: Boosting            | Sequential weighted trees                                                   | Forecast product demand                                 | Features → Demand             | No assumptions                                                                     | Boosts weak learners                                      | Sensitive to noise                                           | ŷ = Σ αₜ treeₜ(X)                  | y                       | X                         | Tree weights αₜ, trees         | n_estimators, learning_rate  | MAE, MSE, RMSE, R², Adj. R², Training Error, Test Error                              | High               | Medium              | ★★★☆☆              | ★★★☆☆         | O(T n)                     | from sklearn.ensemble import AdaBoostRegressor                          | Freund & Schapire, 1996 |
|  11 | Supervised | Regression       | Gradient Boosting (GBM)         | True ML: Boosting            | Gradient descent on residuals                                               | Predict retail sales                                    | Features → Sales              | No assumptions                                                                     | Powerful, flexible                                        | Slow, overfits                                               | ŷ = Σ γₜ treeₜ(X)                  | y                       | X                         | Trees, shrinkage γ             | learning_rate, n_estimators  | MAE, MSE, RMSE, R², Adj. R², Log-Loss (for probabilistic models), Feature importance | High               | Low                 | ★★☆☆☆              | ★★★☆☆         | O(T n log n)               | from sklearn.ensemble import GradientBoostingRegressor                  | Friedman, 2001          |
|  12 | Supervised | Regression       | XGBoost                         | True ML: Boosting            | Regularized GBM with 2nd-order                                              | Predict ad click-through rate (CTR)                     | Features → Click probability  | No assumptions                                                                     | Fast, handles missing, wins Kaggle                        | Complex, memory-heavy                                        | obj = Σ loss + Σ reg(tree)         | y                       | X                         | Trees, weights                 | max_depth, eta, subsample    | MAE, MSE, RMSE, R², Adj. R², Log-Loss (for probabilistic models), Feature importance | High               | Low                 | ★★☆☆☆              | ★★★★★         | O(T n log n)               | import xgboost as xgb                                                   | Chen & Guestrin, 2016   |
|  13 | Supervised | Regression       | LightGBM                        | True ML: Boosting            | Histogram-based, leaf-wise                                                  | Predict fraud score in banking                          | 1M+ rows → Value              | No assumptions                                                                     | Extremely fast, low memory                                | Less control over splits                                     | ŷ = Σ treeₜ(X)                     | y                       | X                         | Trees, histograms              | num_leaves, learning_rate    | MAE, MSE, RMSE, R², Adj. R², Log-Loss (for probabilistic models), Feature importance | High               | Low                 | ★★☆☆☆              | ★★★★★         | O(T n)                     | import lightgbm as lgb                                                  | Ke et al., 2017         |
|  14 | Supervised | Regression       | CatBoost                        | True ML: Boosting            | Ordered boosting for cats                                                   | Predict telecom customer churn                          | Categorical + numeric → Churn | No assumptions                                                                     | Native categorical support                                | Slower startup                                               | ŷ = Σ treeₜ(X)                     | y                       | X                         | Trees, cat embeddings          | depth, cat_features          | MAE, MSE, RMSE, R², Adj. R², Log-Loss (for probabilistic models), Feature importance | High               | Low                 | ★★☆☆☆              | ★★★★☆         | O(T n log n)               | from catboost import CatBoostRegressor                                  | Prokhorenkova, 2018     |