|   # | ML Type       | Algorithm Type   | Algorithm                                                                | Category                     | Category Rationale                             | Real-World Use Case                        | Input → Output           | Assumptions                          | Strengths                                                        | Weaknesses                              | Equation                                               | Dependent variable(s)   | Independent variable(s)   | Model Parameters               | Hyperparameters                           | Evaluation Metrics                                                                                                                                                 | Overfitting Risk   | Underfitting Risk   | Interpretability   | Scalability   | Training Time Complexity   | Python Import                     | Historical Origin                   |
|----:|:--------------|:-----------------|:-------------------------------------------------------------------------|:-----------------------------|:-----------------------------------------------|:-------------------------------------------|:-------------------------|:-------------------------------------|:-----------------------------------------------------------------|:----------------------------------------|:-------------------------------------------------------|:------------------------|:--------------------------|:-------------------------------|:------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------|:--------------------|:-------------------|:--------------|:---------------------------|:----------------------------------|:------------------------------------|
|  42 | Reinforcement | Value-Based      | Q-Learning                                                               | True ML: Tabular RL          | Off-policy TD learning with max operator       | Learn to play Atari (Breakout) from pixels | Game state → Best action | Discrete finite MDP, Markov property | Off-policy (uses best known), simple, converges with exploration | Tabular → curse of dimensionality       | Q(s,a) ← Q(s,a) + α [r + γ max_{a'} Q(s',a') − Q(s,a)] | None                    | s ∈ S, a ∈ A              | Q-table: Q(s,a)                | α, γ, ε (ε-greedy)                        | Cumulative Reward, Convergence Rate (Q-value Stability), Exploration vs Exploitation Balance, Episode Length, Learning Curve (Average Return per Episode)          | Medium             | High (tabular)      | ★★☆☆☆              | ★☆☆☆☆         | O(epochs × |S| × |A|)      | n/a (use numpy + gym)             | Watkins & Dayan, 1992               |
|  43 | Reinforcement | Value-Based      | SARSA (State–Action–Reward–State–Action)                                 | True ML: Tabular RL          | On-policy TD learning                          | Robot learns to walk without falling       | State → Action           | Discrete MDP, policy generates data  | On-policy, safe in real systems                                  | Slower convergence, policy-dependent    | Q(s,a) ← Q(s,a) + α [r + γ Q(s',a') − Q(s,a)]          | None                    | s, a                      | Q-table                        | α, γ, ε                                   | Cumulative Reward, Policy Stability, Convergence Rate, Episode Length, Learning Curve (On-Policy Return)                                                           | Medium             | High (tabular)      | ★★☆☆☆              | ★☆☆☆☆         | O(epochs × |S| × |A|)      | n/a (use numpy + gym)             | Rummery & Niranjan, 1994            |
|  44 | Reinforcement | Value-Based      | DQN (Deep Q-Network)                                                     | True ML: Deep RL             | Q-Learning with neural net + experience replay | Play Atari 2600 games at superhuman level  | Pixels → Action          | Discrete actions, visual input       | Deep + replay → stable, scalable to pixels                       | Unstable without tricks, overestimation | L = (r + γ max Q(s',a';θ⁻) − Q(s,a;θ))²                | None                    | s (pixels), a             | θ (CNN weights), target net θ⁻ | buffer_size, learning_rate, target_update | Average Reward per Episode, Loss Function (TD Error), Q-Value Convergence, Replay Buffer Efficiency, Stability under Different ε-Greedy Policies                   | High               | Low                 | ★☆☆☆☆              | ★★★☆☆         | O(frames × updates)        | from stable_baselines3 import DQN | Mnih et al., 2015                   |
|  45 | Reinforcement | Policy-Based     | Policy Gradient (REINFORCE)                                              | True ML: Policy Optimization | Gradient ascent on expected reward             | Train robotic arm to reach target          | State → Action prob      | Differentiable policy                | Direct policy search, works in continuous                        | High variance, sample-inefficient       | ∇θ J(θ) = E[∇θ log π(a|s;θ) · Gₜ]                      | None                    | s, a                      | Policy π(a|s;θ)                | learning_rate, network_arch               | Average Return, Policy Entropy (Exploration Measure), Gradient Variance, Learning Curve Smoothness, Convergence Stability                                          | High               | Low                 | ★☆☆☆☆              | ★★☆☆☆         | O(episode length × params) | from torch import nn              | Williams, 1992; Sutton et al., 2000 |
|  46 | Reinforcement | Actor-Critic     | A2C / A3C (Advantage Actor–Critic / Asynchronous Advantage Actor–Critic) | True ML: Hybrid RL           | Actor updates policy, Critic estimates value   | Train AI to play 3D games (Doom)           | State → Action           | Stochastic policy                    | Lower variance than REINFORCE, parallelizable                    | Still sample-heavy                      | ∇θ log π(a|s;θ) · (R − V(s;φ))                         | None                    | s, a                      | Actor θ, Critic φ              | n_steps, ent_coef                         | Average Reward, Value Loss and Policy Loss, Entropy Regularization, Advantage Estimate Stability, Training Efficiency (Throughput)                                 | High               | Low                 | ★☆☆☆☆              | ★★★★☆         | O(actors × steps)          | from stable_baselines3 import A2C | Mnih et al., 2016                   |
|  47 | Reinforcement | Actor-Critic     | PPO (Proximal Policy Optimization)                                       | True ML: Hybrid RL           | Clipped surrogate objective for stability      | Train humanoid to walk in MuJoCo           | State → Action           | Trust region                         | Stable, sample-efficient, widely used                            | Tuning clip, complex                    | L = Ê[min(rθ Â, clip(rθ,1-ε,1+ε)Â)]                    | None                    | s, a                      | Actor θ, Critic φ              | clip_range, n_epochs                      | Average Episode Reward, Clipped Objective Value, Policy KL-Divergence, Entropy Coefficient, Value Function Loss, Training Stability (Surrogate Objective Behavior) | High               | Low                 | ★☆☆☆☆              | ★★★★☆         | O(epochs × batch size)     | from stable_baselines3 import PPO | Schulman et al., 2017               |
|  48 | Reinforcement | Monte Carlo      | Monte Carlo Methods                                                      | True ML: Sampling            | Full episode return as unbiased estimate       | Solve Blackjack or simple gridworld        | Episode → Value          | Episodic, full trajectory            | Unbiased, simple                                                 | High variance, needs full episodes      | Gₜ = Rₜ₊₁ + γRₜ₊₂ + … + γᵀ⁻ᵗRₜ                         | None                    | s, a                      | Value V(s) or Q(s,a)           | γ                                         | Average Return (per Episode), Variance of Return Estimates, Convergence Speed, Policy Evaluation Accuracy, Episode Count until Convergence                         | Medium             | High                | ★★★☆☆              | ★★☆☆☆         | O(episodes × length)       | n/a (custom)                      | Sutton, 1988                        |